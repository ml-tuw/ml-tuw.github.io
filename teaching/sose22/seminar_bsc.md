---
layout: entitled
title: Bachelor Seminar Wissenschaftliches Arbeiten
---

## General information

- TISS: [(link)](https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193052&semester=2022S&dswid=6676&dsrid=709)
- contact: [Patrick Indri](mailto:patrick.indri@tuwien.ac.at)
- meeting link: [https://tuwien.zoom.us/my/patrickindri](https://tuwien.zoom.us/my/patrickindri)
- everything important will be announced in TUWEL/TISS.


## Format
This seminar simulates a machine learning conference, where the students take on the role of authors and reviewers. It consists of multiple phases.

### 1. Proposal phase

Attend the **mandatory** first meeting on 22.03.2022, 15:00 (either in person in **Seminarraum 127** in Gußhausstr. 27-29, room CD 03 24, or remotely at **[https://tuwien.zoom.us/my/patrickindri](https://tuwien.zoom.us/my/patrickindri)**).

#### Option 1: our suggestions
 > You select **two** projects/papers (i.e. two bullet points) from one of the topics below. You will work with the material mentioned in the overview and the project-specific resources.   

#### Option 2: your own projects
 > You choose **two** different own project ideas to work on. This can be some existing machine learning paper/work or an own creative idea in the context of machine learning. Importantly, it has to be specific and worked out well.

We can only accept your own proposals if you can answer the mentioned questions and have a well worked out project idea.

**Independent of the option you chose**, understand the fundamentals of your projects and try to answer the following questions:

- **What** is the problem?
- **Why** is it an interesting problem?
- **How** do you plan to approach the problem? /
**How** have the authors of your project approached the problem?

Select projects and write a short description of them together with the answers to the questions (~3 sentences should be sufficient) in **TUWEL**.

We can only accept your own proposals if you can answer the mentioned questions and have a well worked out project idea.

### 2. Bidding and assignment phase
You and your fellow students will act as reviewers and bid on the projects of your peers you want to review. Based on the biddings, we (in the role as chairs of the conference) will select one of each student's proposals as the actual project you will work on for the rest of this semester. You **do not** need to work on the other project, anymore. Additionally, we will also assign two different projects from other students to you, which you will have to review later in the semester. 

### 3. Working phase
Now the actual work starts. Gather deep understanding of your project, write a first draft of your report and give a 5-minute presentation. Feel free to go beyond the given material.

You will schedule two meetings with your supervisor to discuss your progress, but do not hesitate to contact him/her if you have any questions.

### 4. Reviewing phase
You will again act as a reviewer for the conference by writing two reviews, one for each draft report assigned to you.

### 5. Writing phase
Based on the reviews from your peers (and our feedback) you will further work on your project. 

### 6. Submission phase
Give a final presentation and submit your report.

## General resources (freely available books)

- Understanding machine learning: from theory to algorithms. Shai Shalev-Shwartz and Shai Ben-David [(pdf)](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html)
- Foundations of machine learning. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar [(pdf)](https://cs.nyu.edu/~mohri/mlbook/)
- Foundations of data science. Avrim Blum, John Hopcroft, and Ravindran Kannan [(pdf)](https://www.cs.cornell.edu/jeh/book.pdf)
- Mathematics for machine learning. Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong [(pdf)](https://mml-book.github.io/)
- Mining of massive datasets. Jure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman [(pdf)](http://infolab.stanford.edu/~ullman/mmds/book0n.pdf)
- Reinforcement learning: an introduction. Richard Sutton and Andrew Barto [(pdf)](http://incompleteideas.net/book/the-book.html)
- Deep learning and neural networks. Ian Goodfellow and Yoshua Bengio and Aaron Courville [(pdf)](https://www.deeplearningbook.org/)

## Topics (Tentative)
You should have access to the literature and papers through Google scholar, DBLP, the provided links, or the TU library. Feel free to watch the linked talks to get an overview on the topics.

<details>
  <summary><b>Kernels</b> (click to expand)</summary>
<p>Overview:</p>
<ul>
<li>preface and introduction up to section 1.5 of "Learning with kernels" by Bernhard Schölkopf and Alex Smola, 2002 <a href="http://agbs.kyb.tuebingen.mpg.de/lwk/">(pdf)</a>.</li>
<li>introduction to kernels: Bernhard Schölkopf - MLSS 2013 <a href="https://www.youtube.com/watch?v=uzWgB1VO9xQ">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>support vector machines (Bennett and Campbell. "Support vector machines: hype or hallelujah?." ACM SIGKDD 2000)</li>
<li>one class support vector machine (Khan and Madden. "A survey of recent trends in one class classification." Irish conference on artificial intelligence and cognitive science 2009)</li>
<li>string kernels (Lodhi, et al. "Text classification using string kernels." Journal of machine learning research 2002)</li>
<li>kernels for distances (Schölkopf. "The kernel trick for distances." NIPS 2001)</li>
</ul>

</details>


<details>
  <summary><b>Online learning</b> (click to expand)</summary>
<p>Overview:</p>
<ul>
<li>chapter 1 of "A modern introduction to online learning" by Francesco Orabona, 2020.</li>
<li>introduction to online learning (iterative learning / streaming settings): Nicolò Cesa-Bianchi - Mediterranean Machine Learning school 2021 <a href="https://www.youtube.com/watch?v=M6DNMESf5Xk">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>online (sub-)gradient descent (chapter 2 of "A modern introduction to online learning", Francesco Orabona, 2020)</li>
<li>stochastic bandits (introduction and chapter 1 of "Introduction to multi-armed bandits", Aleksandrs Slivkins, 2019)</li>
<li>online learning with expert advice (introduction and chapter 5 of "Introduction to multi-armed bandits", Aleksandrs Slivkins, 2019)</li>
<li>adversarial bandits (introduction and chapter 6 of "Introduction to multi-armed bandits", Aleksandrs Slivkins, 2019)</li>
<li>learning directed cuts (Gärtner and  Garriga. "The cost of learning directed cuts." ECML 2007)</li>
<li>predicting dynamic difficulty (Missura and Gärtner. "Predicting dynamic difficulty." NIPS 2011)</li>
</ul>

</details>

<details>
  <summary><b>Dimensionality reduction</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>chapter 1 and 2 of "Dimension reduction: a guided tour" by Christopher Burges, 2010.</li>
<li>introduction and overview on probabilistic dimensionality reduction: Neil Lawrence - MLSS 2012 <a href="https://www.youtube.com/watch?v=RmjMLeYXDnI">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>principal component analysis (PCA) and singular value decomposition (SVD) (chapter 3 of Foundations of Data Science book)</li>
<li>random projections (chapter 23.2 of "Understanding machine learning" <strong>and</strong> Dasgupta. "Experiments with random projection." UAI 2000)</li>
</ul>

</details>

<details>
  <summary><b>Equivariant neural networks</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>chapter 8 "equivariant neural networks" of "Deep learning for molecules and materials" by Andrew D. White, 2021. <a href="https://whitead.github.io/dmol-book/dl/Equivariant.html">(pdf)</a>.</li>
<li>introduction to equivariance: Taco Cohen and Risi Kondor - Neurips 2020 Tutorial (first half) <a href="https://slideslive.com/38943570/equivariant-networks">(slideslive-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>deep sets (Zaheer, et al. "Deep sets." NIPS 2017)</li>
<li>meta-learning symmetries (Zhou, et al. "Meta-learning symmetries by reparameterization." ICLR 2021)</li>
<li>learning unitary opeartors (Hyland and Rätsch. "Learning unitary operators with help from u (n)." AAAI 2017)</li>
</ul>

</details>

<details>
  <summary><b>Graph neural networks</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>introduction and overview on graph neural networks: Petar Veličković - Tensorflow Tech Talks 2021 <a href="https://www.youtube.com/watch?v=8owQBFAHw7E">(youtube-link)</a></li>
<li>part II "graph representation learning" by William L. Hamilton <a href="https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf">(pdf)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>expressivity of graph neural networks (Xu et al. "How powerful are graph neural networks?" ICLR 2019)</li>
<li>performance of graph neural networks (Dwivedi et al. "Benchmarking graph neural networks." 2020)</li>
</ul>

</details>

<details>
  <summary><b>Graph kernels</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>first 23 pages of "A survey on graph kernels" (Applied Network Science 2019) by Nils M. Kriege, et al.</li>
<li>practical motivation for graph kernels in computational biology: Karsten Borgwardt -  MLSS 2013 (the first 35 minutes) <a href="https://www.youtube.com/watch?v=Id1iOqeJaZY">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>hardness and walk-based kernels (Gärtner, et al. "On graph kernels: hardness results and efficient alternatives." Learning theory and kernel machines 2003)</li>
<li>cyclic pattern kernel (Horváth, et al. "Cyclic pattern kernels for predictive graph mining." ACM SIGKDD 2004)</li>
<li>Weisfeiler-Lehman kernel (Shervashidze, et al. "Weisfeiler-lehman graph kernels." Journal of machine learning research 2011)</li>
</ul>

</details>

<details>
  <summary><b>Knowledge graph embedding</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>"graph representation learning" by William L. Hamilton <a href="https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf">(pdf)</a></li>
<li>Knowledge Graph Embeddings Tutorial: From Theory to Practice, 2020 (https://kge-tutorial-ecai2020.github.io/)</li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>Knowledge Graph Embeddings (focus on deep learning approaches)</li>
<ul>
<li>Q. Wang, Z. Mao, B. Wang, L. Guo. "Knowledge Graph Embedding: A Survey of Approaches and Applications", 2017</li>
<li>Y. Dai, S. Wang, N. Xiong, W. Guo. "A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks", 2020</li>
<li>M. Wang, L. Qiu, X. Wang. "A Survey on Knowledge Graph Embeddings for Link Prediction", 2021</li>
</ul>
</ul>

</details>

<details>
  <summary><b>Semi-supervised learning</b> (click to expand)</summary>
  
<p>Overview:</p>
<ul>
<li>chapter 1/introduction of "Semi-supervised learning" by Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien, 2006 <a href="http://olivier.chapelle.cc/ssl-book/ssl_toc.pdf">(pdf)</a>.</li>
<li>introduction to semi-supervised learning: Tom Mitchell - Carnegie Mellon University 2011 <a href="https://www.youtube.com/watch?v=OMRlnKupsXM">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>graph cuts (Blum and Chawla. "Learning from labeled and unlabeled data using graph mincuts." ICML 2001)</li>
<li>label propagation (Zhu, et al. "Semi-supervised learning using Gaussian fields and harmonic functions." ICML 2003)</li>
<li>learning with local and global consistency (Zhou, et al. "Learning with local and global consistency." NIPS 2004)</li>
<li>semi-supervised learning by entropy minimization (Grandvalet and Bengio. "Semi-supervised learning by entropy minimization." NIPS 2005)</li>
</ul>

</details>

<details>
  <summary><b>Active learning</b> (click to expand)</summary>
  
<p>Overview:</p>
<ul>
<li>chapter 1 "Automating inquiry" of Burr Settles' "Active learning" (AL) book, 2012.</li>
<li>introduction to active learning: Sanjoy Dasgupta - Microsoft 2016 <a href="https://www.youtube.com/watch?v=FE1r7_SQq6Y">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>active learning and label propagation (Zhu, et al. "Combining active learning and semi-supervised learning using Gaussian fields and harmonic functions." ICML 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining 2003.)</li>
<li>hierarchical sampling for active learning "Sanjoy and Hsu. "Hierarchical sampling for active learning." ICML 2008)</li>
</ul>

</details>
