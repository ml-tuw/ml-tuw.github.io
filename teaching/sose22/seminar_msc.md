---
layout: entitled
title: Seminar on Theoretical Aspects of Machine Learning Algorithms
---
## General information

- TISS: [(link)](https://tiss.tuwien.ac.at/course/courseDetails.xhtml?dswid=9375&dsrid=649&courseNr=194118&semester=2022S)
- contact: [Maximilian Thiessen](mailto:maximilian.thiessen@tuwien.ac.at)
- meeting link: [https://tuwien.zoom.us/my/maxthiessen](https://tuwien.zoom.us/my/maxthiessen)
- everything important will be announced in TUWEL/TISS.


## Format
This seminar simulates a machine learning conference, where the students take on the role of authors and reviewers. It consists of multiple phases. 

### 1. Proposal phase

Attend the **mandatory** first meeting on 07.03, 15:00 (either in person at **Gußhausstraße 27-29 CA 03 13**, or remotely at **[https://tuwien.zoom.us/my/maxthiessen](https://tuwien.zoom.us/my/maxthiessen)**).

#### Option 1: our suggestions
 > You select **two** projects/papers (i.e., two bullet points) from one of the topics below. You will work with the material mentioned in the overview and the project-specific resources.   

#### Option 2: your own projects
 > You choose **two** different own project ideas to work on. This can be some existing machine learning paper/work or an own creative idea in the context of machine learning. Importantly, it has to be specific and worked out well.

**Independent of the option you chose**, understand the fundamentals of your projects and try to answer the following questions:

- **What** is the problem?
- **Why** is it an interesting problem?
- **How** do you plan to approach the problem? /
**How** have the authors of your project approached the problem?

Select projects and write a short description of them together with the answers to the questions (~3 sentences shoud be sufficient) in **TUWEL**.

We can only accept your own proposals if you can answer the mentioned questions and have a well worked out project idea.

### 2. Bidding and assignment phase
You will also act as reviewers and bid on the projects of your peers you want to review. Based on the biddings, we (in the role as chairs of the conference) will select one of each student's proposals as the actual project you will work on for the rest of this semester. You **do not** need to work on the other project, anymore. Additionally, we will also assign two different projects from other students to you, which you will have to review later in the semester. 

### 3. Working phase
Now the actual work starts. Gather deep understanding of your project, write a first draft of your report and give a 5-minute presentation. We recommend to **go beyond** the given material.

### 4. Reviewing phase
You will again act as a reviewer for the conference by writing two reviews, one for each draft report assigned to you.

### 5. Writing phase
Based on the reviews from your peers (and our feedback) you will further work on your project. 

### 6. Submission phase
Give a final presentation and submit your report.

## General resources (freely available books and lecture notes)

- Understanding machine learning: from theory to algorithms. Shai Shalev-Shwartz and Shai Ben-David [(pdf)](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html)
- Foundations of machine learning. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar [(pdf)](https://cs.nyu.edu/~mohri/mlbook/)
- Foundations of data science. Avrim Blum, John Hopcroft, and Ravindran Kannan [(pdf)](https://www.cs.cornell.edu/jeh/book.pdf)
- Mathematics for machine learning. Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong [(pdf)](https://mml-book.github.io/)
- Mining of massive datasets. Jure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman [(pdf)](http://infolab.stanford.edu/~ullman/mmds/book0n.pdf)
- Reinforcement learning: an introduction. Richard Sutton and Andrew Barto [(pdf)](http://incompleteideas.net/book/the-book.html)
- Research Methods in Machine Learning. Tom Dietterich [(pdf)](http://web.engr.oregonstate.edu/~tgd/talks/new-in-ml-2019.pdf)

## Topics (Tentative)
You should have access to the literature and papers through Google scholar, DBLP, the provided links, or the TU library.

<details>
  <summary><b>Knowledge Graph Embeddings</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>"graph representation learning" by William L. Hamilton <a href="https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf">(pdf)</a></li>
<li>Knowledge Graph Embeddings Tutorial: From Theory to Practice, 2020 (https://kge-tutorial-ecai2020.github.io/)</li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>Knowledge Graph Embeddings (focus on deep learning approaches)
<ul>
<li>Q. Wang, Z. Mao, B. Wang, L. Guo. "Knowledge Graph Embedding: A Survey of Approaches and Applications", 2017</li>
<li>Y. Dai, S. Wang, N. Xiong, W. Guo. "A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks", 2020</li>
<li>M. Wang, L. Qiu, X. Wang. "A Survey on Knowledge Graph Embeddings for Link Prediction", 2021</li>
</ul>
</li>
</ul>

</details>

<details>
  <summary><b>Neurosymbolic AI / Logic & ML</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>Neurosymbolic AI: The 3rd Wave, 2020 (A. Garcez, L. Lamb)</li>
<li>Neural-Symbolic Cognitive Reasoning, 2009 (A. Garcez, L. Lamb)</li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>find your own topic :) (a starting point can be the survey from L. De Raedt, S. Dumancic, R. Manhaeve, G. Marra. "From Statistical Relational to Neuro-Symbolic Artificial Intelligence", 2020)</li>
<li>SAT solving using deep learning
<ul>
<li>D. Selsam, M. Lamm, B. Bünz, P. Liang, D. Dill, L. de Moura. "Learning a SAT Solver from Single-Bit Supervision", 2019</li>
<li>V. Kurin, S. Godil, S. Whiteson, B. Catanzaro. "Improving SAT Solver Heuristics with Graph Networks and Reinforcement Learning", 2019</li>
<li>J. You, H. Wu, C. Barrett, R. Ramanujan, J. Leskovec. "G2SAT: Learning to Generate SAT Formulas", 2019</li>
</ul>
</li>
</ul>

</details>

<details>
  <summary><b>Submodularity in machine learning</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>chapter 1-3 of "Learning with submodular functions: a convex optimization perspective" by Francis Bach, 2013.</li>
<li>introduction to submodularity in machine learning: Stefanie Jegelka - MLSS 2017  <a href="https://www.youtube.com/watch?v=umA8QzY5C54">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>submodularity in data subset selection and active learning (Wei, et al. "Submodularity in data subset selection and active learning." ICML 2015)</li>
<li>robust submodular observation selection (Krause, et al. "Robust submodular observation selection." Journal of machine learning research 2008)</li>
<li>submodular function maximization (Krause and Golovin. "Submodular function maximization." 2014)</li>
<li>graph cuts for image segmentation (Blum and Chawla. "Learning from labeled and unlabeled data using graph mincuts." ICML 2001 <strong>and</strong> Jegelka and Bilmes. "Submodularity beyond submodular energies: coupling edges in graph cuts." CVPR 2011)</li>
<li>learning submodular functions (Balcan and Harvey. "Learning submodular functions." ACM symposium on theory of computing 2011)</li>
<li>batch active learning using submodular optimization (Chen and Krause. "Near-optimal batch mode active learning and adaptive submodular optimization." ICML 2013)</li>
</ul>

</details>


<details>
  <summary><b>Graph kernels</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>first 23 pages of "A survey on graph kernels" (Applied Network Science 2019) by Nils M. Kriege, et al.</li>
<li>practical motivation for graph kernels in computational biology: Karsten Borgwardt -  MLSS 2013 (the first 35 minutes) <a href="https://www.youtube.com/watch?v=Id1iOqeJaZY">(youtube-link)</a></li>
</ul>
<p>Papers and topics:</p>
<ul>
<li>hardness and expressivity (Gärtner, et al. "On graph kernels: Hardness results and efficient alternatives." COLT 2003 <strong>and</strong> Ramon and Gärtner. "Expressivity versus efficiency of graph kernels." Workshop on mining graphs, trees and sequences 2003)</li>
<li>(k-dimensional) Weisfeiler-Lehman kernel (Shervashidze, et al. "Weisfeiler-Lehman graph kernels." Journal of machine learning research 2011 <strong>and</strong> Morris, et al. "Glocalized Weisfeiler-Lehman graph kernels: Global-local feature maps of graphs." ICDM 2017)</li>
<li>mutiple and deep graph kernel learning (Aiolli, et al. "Multiple graph-kernel learning" <strong>and</strong> Yanardag and Vishwanathan. "Deep graph kernels." SIGKDD 2015)</li>
</ul>

</details>

<details>
  <summary><b>Kernel methods</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>chapters 1 and 2 of "Learning with kernels" by Bernhard Schölkopf and Alex Smola, 2002 <a href="http://agbs.kyb.tuebingen.mpg.de/lwk/">(pdf)</a></li>
<li>introduction to kernels: Bernhard Schölkopf - MLSS 2013 <a href="https://www.youtube.com/watch?v=uzWgB1VO9xQ">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>Nyström method (Drineas and Mahoney. "On the Nyström method for approximating a Gram matrix for improved kernel-based learning." Journal of machine learning research 2005 <strong>and</strong> Kumar, et al. "Sampling methods for the Nyström method." Journal of machine learning research 2012)</li>
<li>Nyström method with kernel k-means++ samples as landmarks (Drineas and Mahoney. "On the Nyström method for approximating a Gram matrix for improved kernel-based learning." Journal of machine learning research 2005 <strong>and</strong> Oglic and Gärtner. "Nyström method with kernel k-means++ samples as landmarks."  ICML 2017)</li>
<li>random features (Rahimi and Recht. "Random features for large-scale kernel machines." NIPS 2007 <strong>and</strong> Le, et al. "Fastfood: approximate kernel expansions in loglinear time." ICML 2013)</li>
<li>neural tangent kernel (Jacot, et al. "Neural tangent kernel: convergence and generalization in neural networks." NIPS 2018)</li>
</ul>

</details>


<details>
  <summary><b>Semi-supervised learning</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>first chapter/introduction of "Semi-supervised learning" (SSL) by Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien, 2006 <a href="http://olivier.chapelle.cc/ssl-book/ssl_toc.pdf">(pdf)</a></li>
<li>introduction to semi-supervised learning: Tom Mitchell - Carnegie Mellon University 2011 <a href="https://www.youtube.com/watch?v=OMRlnKupsXM">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>transductive support vector machines (chapter 6 in SSL by Thorsten Joachims)</li>
<li>large-margin semi-supervised learning (Wang, et al. "On efficient large margin semisupervised learning: method and theory." Journal of machine learning research 2009)</li>
<li>PAC model for semi-supervised learning (chapter 22 of SSL by Maria-Florina Balcan and Avrim Blum)</li>
<li>generalization error bounds (Rigollet. "Generalization error bounds in semi-supervised classification under the cluster assumption." Journal of machine learning research 2007)</li>
<li>regularization and semi-supervised learning on graphs (Belkin, et al. "Regularization and semi-supervised learning on large graphs." COLT 2004)</li>
<li>manifold regularization (Belkin, et al. "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples." Journal of machine learning research 2006)</li>
<li>label propagation (Zhu, et al. "Semi-supervised learning using Gaussian fields and harmonic functions." ICML 2003 <strong>and</strong> Zhou, et al. "Learning with local and global consistency." NIPS 2004)</li>
<li>normalized cuts (Shi and Malik "Normalized cuts and image segmentation." IEEE TPAMI Journal 2000 <strong>and</strong> Joachims "Transductive learning via spectral graph partitioning." AAAI 2003)</li>
</ul>

</details>

<details>
  <summary><b>Active learning</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>chapter 1 "Automating inquiry" of Burr Settles' "Active learning" book, 2012.</li>
<li>introduction and recent research: Rob Nowak and Steve Hanneke - ICML 2019 tutorial <a href="https://youtube.videoken.com/embed/0TADiY7iPAc">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>active learning with graph cuts (Blum and Chawla. "Learning from labeled and unlabeled data using graph mincuts." ICML 2001 <strong>and</strong> Guillory and Bilmes. "Label selection on graphs." NIPS 2009):</li>
<li>agnostic/noisy active learning (Balcan, et al. "Agnostic active learning." Journal of computer and system sciences 2009 <strong>and</strong> Beygelzimer, et al. "Importance weighted active learning.")</li>
<li>active nearest-neighbour learning (Kontorovich, et al. "Active nearest-neighbor learning in metric spaces." Journal of machine learning research 2017)</li>
<li>active learning on trees and graphs (Cesa-Bianchi, et al. "Active learning on trees and graphs", COLT 2013)</li>
<li>shortest-path-based active learning (Dasarathy, et al. "S2: an efficient graph based active learning algorithm with application to nonparametric classification." COLT 2015)</li>
</ul>

</details>

<details>
  <summary><b>Online learning</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>chapter 1 of "A modern introduction to online learning" by Francesco Orabona, 2020.</li>
<li>introduction to online learning (iterative learning / streaming settings): Nicolò Cesa-Bianchi - Mediterranean Machine Learning school 2021 <a href="https://www.youtube.com/watch?v=M6DNMESf5Xk">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>weighed majority and Littlestone dimension (Littlestone and Warmuth. "The weighted majority algorithm." Information and computation 1994 <strong>and</strong> Littlestone "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm." Machine Learning 1988).</li>
<li>online (sub-)gradient descent (chapter 2-4 of "A modern introduction to online learning", Francesco Orabona, 2020)</li>
<li>bandits and expert advice (introduction and chapter 1,5,6 of "Introduction to multi-armed bandits", Aleksandrs Slivkins, 2019)</li>
<li>(online) learning with partial orders (Gärtner and  Garriga. "The cost of learning directed cuts." ECML 2007 <strong>and</strong> Missura and Gärtner. "Predicting dynamic difficulty." NIPS 2011)</li>
</ul>

</details>


<details>
  <summary><b>Clustering and dimensionality reduction</b> (click to expand)</summary>

<p>Overview:</p>
<ul>
<li>chapter 1 and 2 of "Dimension reduction: a guided tour" by Christopher Burges, 2010, <strong>and</strong> chapter 22 (the introduction section before 22.1 and section 22.5) of "Understanding machine learning".</li>
<li>introduction and theoretical overview on clustering: Shai Ben-David Cheriton Symposium 2017 <a href="https://www.youtube.com/watch?v=Pq5d1Y2YpgA">(youtube-link)</a></li>
<li>introduction and overview on probabilistic dimensionality reduction: Neil Lawrence - MLSS 2012 <a href="https://www.youtube.com/watch?v=RmjMLeYXDnI">(youtube-link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>kernel PCA and multidimensional scaling (Schölkopf, et al. "Kernel principal component analysis." ICANN 1997 <strong>and</strong> Williams "On a connection between kernel PCA and metric multidimensional scaling." Machine learning 2002)</li>
<li>spectral clustering (Von Luxburg. "A tutorial on spectral clustering." Statistics and computing 2007)</li>
<li>(adaptive) correlation clustering (Bansal, et al. "Correlation clustering." Machine learning 2004 <strong>and</strong> Bressan, Marco, et al. "Correlation clustering with adaptive similarity queries." NeurIPS 2019)</li>
<li>(approximate) k-means++ (Arthur and Vassilvitskii. "k-means++: The advantages of careful seeding." Stanford, 2006 <strong>and</strong> Bachem, Olivier, et al. "Approximate k-means++ in sublinear time." AAAI 2016)</li>
<li>clustering under approximation stability (Balcan, et al. "Clustering under approximation stability." Journal of the ACM 2013)</li>
<li>auto-encoders and generative adversarial nets (Diederik and Welling "Auto-encoding variational Bayes" ICLR 2014 <strong>and</strong> Goodfellow, et al. "Generative adversarial nets" NIPS 2014 <strong>and</strong> Tolstikhin, et al. "Wasserstein auto-encoders" ICLR 2018)</li>
</ul>

</details>


<details>
  <summary><b>Modern aspects of learning theory</b> (click to expand)</summary>
<p>Overview:</p>
<ul>
<li>Olivier Bousquet Stéphane Boucheron, and Gábor Lugosi: "Introduction to Statistical Learning Theory" 2003.</li>
<li>Chapters 1-6 of "Understanding machine learning" </li>
<li> "Extending Generalization Theory Towards Addressing Modern Challenges in ML" by Shay Moran, talk at the HUJI ML Club, 2021 <a href="https://www.youtube.com/watch?v=E6Umv6XBJck">(youtube-link)</a></li>
<li> (Basic material) Statistical Machine Learning by Ulrike von Luxburg (we recommend part 38-41) <a href="https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC">(youtube playlist)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
<li>partial concept classes (Alon, et al., "A theory of PAC learnability of partial concept classes", unpublished arXiv:2107.08444)</li>
<li>tight bounds (Bousquet, et al., "Proper learning, Helly number, and an optimal SVM bound" COLT 2020)</li>
<li>universal learning (Bousquet, et al., "A theory of universal learning" STOC 2021)</li>
<li>sample compression schemes (Moran, et al., "Sample compression schemes for VC classes" Journal of the ACM 2016).</li>
<li>generalization bounds for deep neural networks (G.K. Dziugaite, D.M. Roy, "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", 2017)</li>
</ul>
</details>

<details>
  <summary><b>Explainable AI</b> (click to expand)</summary>
  
<p>Overview:</p>
<ul>
  <li>Došilović, Filip Karlo, Mario Brčić, and Nikica Hlupić. "Explainable artificial intelligence: A survey." MIPRO 2018</li>
  <li> Samek, Wojciech, and Klaus-Robert Müller. "Towards explainable artificial intelligence." Explainable AI: interpreting, explaining and visualizing deep learning." Springer, Cham, 2019 </li>
</ul>
<p>Papers and projects:</p>
<ul>
  <li>interpreting model predictions with SHAP and LIME (Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. ""Why should i trust you?" Explaining the predictions of any classifier." ACM SIGKDD 2016 <b>and</b> Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." NIPS 2017</li>
  <li>nonlinear classifiers (Montavon, Grégoire, et al. "Explaining nonlinear classification decisions with deep taylor decomposition." Pattern recognition, 2017)</li>
</ul>

</details>

<details>
  <summary><b>Differential privacy</b> (click to expand)</summary>
  
<p>Overview:</p>
<ul>
  <li>Dwork, Cynthia. "Differential privacy: A survey of results." International conference on theory and applications of models of computation. Springer, 2008)</li>
  <li> Chapter 2 of: Dwork, Cynthia, and Aaron Roth. "The algorithmic foundations of differential privacy." Found. Trends Theor. Comput. Sci. 9.3-4 2014 </li>
</ul>
<p>Papers and projects:</p>
<ul>
  <li>differential privacy and deep learning (Chen, Xiangyi, Steven Z. Wu, and Mingyi Hong. "Understanding gradient clipping in private SGD: A geometric perspective." NeurIPS 2020)</li>
  <li>(extensions of) gaussian mechanism (Balle, Borja, and Yu-Xiang Wang. "Improving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising." International Conference on Machine Learning. PMLR, 2018)</li>
</ul>

</details>

<details>
  <summary><b>Neuro-inspired DL</b> (click to expand)</summary>
<p>Overview:</p>
<ul>
  <li>Spike-timing dependent plasticity <a href="http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity">(link)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
  <li>Spiking neural networks:</li>
  <ul>
    <li>B. Confavreux, F. Zenke, E.J. Agnes, T. Lillicrap, T.P. Vogels. "A meta-learning approach to (re)discover plasticity rules that carve a desired function into a neural network", 2020</li>
    <li>F. Zenke, S. Ganguli. "Superspike: Supervised learning in multilayer spiking neural networks", 2018</li>
  </ul>
  <li>Feedback alignment:</li>
  <ul>
    <li>M. Refinetti et al. "Align, then memorise: the dynamics of learning with feedback alignment", 2021</li>
    <li>J.M. Murray: "Local online learning in recurrent networks with random feedback", 2019</li>
  </ul>
</ul>

</details>

<details>
  <summary><b>Optimization (and Generalization) in Neural Networks</b> (click to expand)</summary>
<p>Overview:</p>
<ul>
  <li>A. Globerson: How SGD Can Succeed Despite Non-Convexity and Over-Parameterization <a href="https://simons.berkeley.edu/sites/default/files/docs/9983/simonsjune18.pdf">(slides)</a></li>
</ul>
<p>Papers and projects:</p>
<ul>
  <li>A. Brutzkus et al: "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", 2017</li>
  <li>Choose one or more papers listed on page 14 in the above mentioned <a href="https://simons.berkeley.edu/sites/default/files/docs/9983/simonsjune18.pdf">slides</a> :)</li>
  <li>A. Shevchenko, M. Mondelli. "Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks", 2020</li>
  <li>H. Petzka et al. "Relative Flatness and Generalization", 2021</li>
</ul>
</details>
